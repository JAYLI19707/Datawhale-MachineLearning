{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4eb6f9-c641-4ed0-b4ff-549e5995f188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:37:22.956620Z",
     "iopub.status.busy": "2025-07-03T15:37:22.956506Z",
     "iopub.status.idle": "2025-07-03T15:37:27.432195Z",
     "shell.execute_reply": "2025-07-03T15:37:27.431680Z",
     "shell.execute_reply.started": "2025-07-03T15:37:22.956607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in d:\\anaconda\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "818bfa3a-c87e-4990-96d8-745291e61064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:37:27.433546Z",
     "iopub.status.busy": "2025-07-03T15:37:27.433176Z",
     "iopub.status.idle": "2025-07-03T15:37:28.404725Z",
     "shell.execute_reply": "2025-07-03T15:37:28.404283Z",
     "shell.execute_reply.started": "2025-07-03T15:37:27.433525Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16ba47ba-b3fe-474b-aceb-0886694516a4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-07-03T15:37:28.405531Z",
     "iopub.status.busy": "2025-07-03T15:37:28.405258Z",
     "iopub.status.idle": "2025-07-03T15:37:36.895694Z",
     "shell.execute_reply": "2025-07-03T15:37:36.895199Z",
     "shell.execute_reply.started": "2025-07-03T15:37:28.405515Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6.67 s\n",
      "Wall time: 6.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1. 数据加载\n",
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./testA_data.csv')\n",
    "submit = test_df[['did']]\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# 2. 时间特征工程\n",
    "for df in [train_df, test_df]:\n",
    "    # 转换为时间戳\n",
    "    df['ts'] = pd.to_datetime(df['common_ts'], unit='ms')\n",
    "    \n",
    "    # 提取时间特征\n",
    "    df['day'] = df['ts'].dt.day\n",
    "    df['dayofweek'] = df['ts'].dt.dayofweek\n",
    "    df['hour'] = df['ts'].dt.hour\n",
    "\n",
    "# 对 full_df 进行同样的处理，用于计算全局聚合特征\n",
    "full_df['ts'] = pd.to_datetime(full_df['common_ts'], unit='ms')\n",
    "full_df['day'] = full_df['ts'].dt.day\n",
    "full_df['dayofweek'] = full_df['ts'].dt.dayofweek\n",
    "full_df['hour'] = full_df['ts'].dt.hour\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e1afb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 21.4 s\n",
      "Wall time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3. RFM 特征工程\n",
    "\n",
    "# 3.1 RFM 特征构建\n",
    "# 我们在 full_df 上计算这些聚合特征，因为它包含了所有用户的所有行为\n",
    "max_ts = full_df['ts'].max()\n",
    "rfm_agg = full_df.groupby('did').agg({\n",
    "    'ts': lambda x: (max_ts - x.max()).days, # Recency\n",
    "    'eid': 'count', # Frequency\n",
    "    'mid': 'nunique', # 行为深度\n",
    "    'common_ts': ['min', 'max'] # 首次和末次行为时间\n",
    "})\n",
    "\n",
    "# 3.2 列名扁平化处理\n",
    "# 将多级索引 ('common_ts', 'min') 合并为单级 'common_ts_min'\n",
    "rfm_agg.columns = ['_'.join(col).strip() for col in rfm_agg.columns.values]\n",
    "rfm_agg = rfm_agg.reset_index()\n",
    "\n",
    "# 3.3 特征重命名\n",
    "rfm_agg.rename(columns={\n",
    "    'ts_<lambda>': 'recency',\n",
    "    'eid_count': 'frequency',\n",
    "    'mid_nunique': 'mid_nunique',\n",
    "    'common_ts_min': 'first_action_ts',\n",
    "    'common_ts_max': 'last_action_ts'\n",
    "}, inplace=True)\n",
    "\n",
    "# 3.4 派生新特征\n",
    "# 计算首次和末次行为的时间跨度（单位：秒）\n",
    "rfm_agg['action_timespan_seconds'] = (rfm_agg['last_action_ts'] - rfm_agg['first_action_ts']) / 1000\n",
    "\n",
    "# 3.5 合并RFM特征到 train_df 和 test_df\n",
    "# 为了确保后续流程的变量一致性，我们直接在 train_df 和 test_df 上合并\n",
    "train_df = pd.merge(train_df, rfm_agg, on='did', how='left')\n",
    "test_df = pd.merge(test_df, rfm_agg, on='did', how='left')\n",
    "\n",
    "# 3.6 清理不再需要的 ts 列\n",
    "for df in [train_df, test_df]:\n",
    "    df.drop(['ts'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6949437-9ef0-4b4d-97ec-e8e38fad3d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:37:36.896472Z",
     "iopub.status.busy": "2025-07-03T15:37:36.896243Z",
     "iopub.status.idle": "2025-07-03T15:37:37.720932Z",
     "shell.execute_reply": "2025-07-03T15:37:37.720502Z",
     "shell.execute_reply.started": "2025-07-03T15:37:36.896454Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重叠 did 数量: 192393\n",
      "占 train 比例: 0.7104 (192393/270837)\n",
      "占 test 比例: 0.9324 (192393/206342)\n",
      "CPU times: total: 1.58 s\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################### 简单分析\n",
    "# 获取 train 和 test 中唯一的 did\n",
    "train_dids = set(train_df['did'].unique())\n",
    "test_dids = set(test_df['did'].unique())\n",
    "\n",
    "# 计算交集\n",
    "overlap_dids = train_dids & test_dids\n",
    "\n",
    "# 数量统计\n",
    "num_overlap = len(overlap_dids)\n",
    "num_train = len(train_dids)\n",
    "num_test = len(test_dids)\n",
    "\n",
    "# 占比\n",
    "ratio_in_train = num_overlap / num_train if num_train > 0 else 0\n",
    "ratio_in_test = num_overlap / num_test if num_test > 0 else 0\n",
    "\n",
    "# 输出结果\n",
    "print(f\"重叠 did 数量: {num_overlap}\")\n",
    "print(f\"占 train 比例: {ratio_in_train:.4f} ({num_overlap}/{num_train})\")\n",
    "print(f\"占 test 比例: {ratio_in_test:.4f} ({num_overlap}/{num_test})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baf134ea-e601-4556-9858-303f040d74ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:37:37.721615Z",
     "iopub.status.busy": "2025-07-03T15:37:37.721406Z",
     "iopub.status.idle": "2025-07-03T15:37:57.681327Z",
     "shell.execute_reply": "2025-07-03T15:37:57.680777Z",
     "shell.execute_reply.started": "2025-07-03T15:37:37.721602Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 需要编码的特征列表\n",
    "cat_features = [\n",
    "    'device_brand', 'ntt', 'operator', 'common_country',\n",
    "    'common_province', 'common_city', 'appver', 'channel',\n",
    "    'os_type', 'udmap'\n",
    "]\n",
    "# 初始化编码器字典\n",
    "label_encoders = {}\n",
    "\n",
    "for feature in cat_features:\n",
    "    # 创建编码器，将类别特征转为0-N的自然数\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # 合并训练集和测试集的所有类别\n",
    "    all_values = pd.concat([train_df[feature], test_df[feature]]).astype(str)\n",
    "    \n",
    "    # 训练编码器（使用所有可能值）\n",
    "    le.fit(all_values)\n",
    "    \n",
    "    # 保存编码器\n",
    "    label_encoders[feature] = le\n",
    "    \n",
    "    # 应用编码\n",
    "    train_df[feature] = le.transform(train_df[feature].astype(str))\n",
    "    test_df[feature] = le.transform(test_df[feature].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7bc120b-7fa4-45b5-8c56-097cddaa8bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:37:57.682944Z",
     "iopub.status.busy": "2025-07-03T15:37:57.682669Z",
     "iopub.status.idle": "2025-07-03T15:37:57.971949Z",
     "shell.execute_reply": "2025-07-03T15:37:57.971479Z",
     "shell.execute_reply.started": "2025-07-03T15:37:57.682926Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 172 ms\n",
      "Wall time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 基础特征 + 目标编码特征 + 聚合特征\n",
    "features = [\n",
    "    # 原始特征\n",
    "    'mid', 'eid', 'device_brand', 'ntt', 'operator', \n",
    "    'common_country', 'common_province', 'common_city',\n",
    "    'appver', 'channel', 'os_type', 'udmap',\n",
    "    # 时间特征\n",
    "    'hour', 'dayofweek', 'day', 'common_ts',\n",
    "    # RFM特征\n",
    "   'recency', 'frequency', 'mid_nunique', 'first_action_ts', 'last_action_ts', 'action_timespan_seconds'\n",
    "]\n",
    "\n",
    "# 准备训练和测试数据\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['is_new_did']\n",
    "X_test = test_df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb98dd6b-bece-4f83-988f-40787fc136c9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-07-03T15:37:57.972634Z",
     "iopub.status.busy": "2025-07-03T15:37:57.972438Z",
     "iopub.status.idle": "2025-07-03T15:43:34.207362Z",
     "shell.execute_reply": "2025-07-03T15:43:34.206910Z",
     "shell.execute_reply.started": "2025-07-03T15:37:57.972621Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始模型训练...\n",
      "\n",
      "======= Fold 1/5 =======\n",
      "[100]\ttraining's binary_logloss: 0.159146\tvalid_1's binary_logloss: 0.159971\n",
      "[200]\ttraining's binary_logloss: 0.14492\tvalid_1's binary_logloss: 0.146302\n",
      "[300]\ttraining's binary_logloss: 0.135447\tvalid_1's binary_logloss: 0.137377\n",
      "[400]\ttraining's binary_logloss: 0.127709\tvalid_1's binary_logloss: 0.130072\n",
      "[500]\ttraining's binary_logloss: 0.122082\tvalid_1's binary_logloss: 0.12488\n",
      "[600]\ttraining's binary_logloss: 0.116762\tvalid_1's binary_logloss: 0.119998\n",
      "[700]\ttraining's binary_logloss: 0.112257\tvalid_1's binary_logloss: 0.115979\n",
      "[800]\ttraining's binary_logloss: 0.108049\tvalid_1's binary_logloss: 0.11213\n",
      "[900]\ttraining's binary_logloss: 0.104356\tvalid_1's binary_logloss: 0.108808\n",
      "[1000]\ttraining's binary_logloss: 0.100591\tvalid_1's binary_logloss: 0.105414\n",
      "Fold 1 Optimal Threshold: 0.4000\n",
      "Fold 1 F1 Score: 0.85934\n",
      "\n",
      "======= Fold 2/5 =======\n",
      "[100]\ttraining's binary_logloss: 0.159565\tvalid_1's binary_logloss: 0.160226\n",
      "[200]\ttraining's binary_logloss: 0.144682\tvalid_1's binary_logloss: 0.145972\n",
      "[300]\ttraining's binary_logloss: 0.13553\tvalid_1's binary_logloss: 0.137423\n",
      "[400]\ttraining's binary_logloss: 0.128458\tvalid_1's binary_logloss: 0.130788\n",
      "[500]\ttraining's binary_logloss: 0.123118\tvalid_1's binary_logloss: 0.126058\n",
      "[600]\ttraining's binary_logloss: 0.117707\tvalid_1's binary_logloss: 0.121157\n",
      "[700]\ttraining's binary_logloss: 0.11308\tvalid_1's binary_logloss: 0.116978\n",
      "[800]\ttraining's binary_logloss: 0.108677\tvalid_1's binary_logloss: 0.113006\n",
      "[900]\ttraining's binary_logloss: 0.105063\tvalid_1's binary_logloss: 0.109855\n",
      "[1000]\ttraining's binary_logloss: 0.101478\tvalid_1's binary_logloss: 0.106743\n",
      "Fold 2 Optimal Threshold: 0.4000\n",
      "Fold 2 F1 Score: 0.85533\n",
      "\n",
      "======= Fold 3/5 =======\n",
      "[100]\ttraining's binary_logloss: 0.160272\tvalid_1's binary_logloss: 0.16018\n",
      "[200]\ttraining's binary_logloss: 0.14529\tvalid_1's binary_logloss: 0.145906\n",
      "[300]\ttraining's binary_logloss: 0.13607\tvalid_1's binary_logloss: 0.137284\n",
      "[400]\ttraining's binary_logloss: 0.128414\tvalid_1's binary_logloss: 0.130167\n",
      "[500]\ttraining's binary_logloss: 0.122705\tvalid_1's binary_logloss: 0.125037\n",
      "[600]\ttraining's binary_logloss: 0.117583\tvalid_1's binary_logloss: 0.120356\n",
      "[700]\ttraining's binary_logloss: 0.112627\tvalid_1's binary_logloss: 0.115828\n",
      "[800]\ttraining's binary_logloss: 0.108535\tvalid_1's binary_logloss: 0.112178\n",
      "[900]\ttraining's binary_logloss: 0.104667\tvalid_1's binary_logloss: 0.108746\n",
      "[1000]\ttraining's binary_logloss: 0.101253\tvalid_1's binary_logloss: 0.105791\n",
      "Fold 3 Optimal Threshold: 0.4000\n",
      "Fold 3 F1 Score: 0.85882\n",
      "\n",
      "======= Fold 4/5 =======\n",
      "[100]\ttraining's binary_logloss: 0.159784\tvalid_1's binary_logloss: 0.160911\n",
      "[200]\ttraining's binary_logloss: 0.144991\tvalid_1's binary_logloss: 0.146783\n",
      "[300]\ttraining's binary_logloss: 0.135879\tvalid_1's binary_logloss: 0.138269\n",
      "[400]\ttraining's binary_logloss: 0.128906\tvalid_1's binary_logloss: 0.131738\n",
      "[500]\ttraining's binary_logloss: 0.12268\tvalid_1's binary_logloss: 0.126035\n",
      "[600]\ttraining's binary_logloss: 0.117804\tvalid_1's binary_logloss: 0.121618\n",
      "[700]\ttraining's binary_logloss: 0.113473\tvalid_1's binary_logloss: 0.117722\n",
      "[800]\ttraining's binary_logloss: 0.109656\tvalid_1's binary_logloss: 0.114329\n",
      "[900]\ttraining's binary_logloss: 0.105786\tvalid_1's binary_logloss: 0.110833\n",
      "[1000]\ttraining's binary_logloss: 0.102048\tvalid_1's binary_logloss: 0.107421\n",
      "Fold 4 Optimal Threshold: 0.4000\n",
      "Fold 4 F1 Score: 0.85543\n",
      "\n",
      "======= Fold 5/5 =======\n",
      "[100]\ttraining's binary_logloss: 0.159174\tvalid_1's binary_logloss: 0.159876\n",
      "[200]\ttraining's binary_logloss: 0.144748\tvalid_1's binary_logloss: 0.145901\n",
      "[300]\ttraining's binary_logloss: 0.13549\tvalid_1's binary_logloss: 0.137127\n",
      "[400]\ttraining's binary_logloss: 0.128071\tvalid_1's binary_logloss: 0.130244\n",
      "[500]\ttraining's binary_logloss: 0.122174\tvalid_1's binary_logloss: 0.124795\n",
      "[600]\ttraining's binary_logloss: 0.117035\tvalid_1's binary_logloss: 0.120162\n",
      "[700]\ttraining's binary_logloss: 0.11233\tvalid_1's binary_logloss: 0.115908\n",
      "[800]\ttraining's binary_logloss: 0.108046\tvalid_1's binary_logloss: 0.11203\n",
      "[900]\ttraining's binary_logloss: 0.104403\tvalid_1's binary_logloss: 0.108845\n",
      "[1000]\ttraining's binary_logloss: 0.100822\tvalid_1's binary_logloss: 0.105656\n",
      "Fold 5 Optimal Threshold: 0.4000\n",
      "Fold 5 F1 Score: 0.85836\n",
      "CPU times: total: 49min 52s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 6. F1阈值优化函数\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    \"\"\"寻找最大化F1分数的阈值\"\"\"\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for threshold in [0.1,0.15,0.2,0.25,0.3,0.35,0.4]:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "# 7. 模型训练与交叉验证\n",
    "import time\n",
    "# 动态生成随机种子（基于当前时间）\n",
    "seed = int(time.time()) % 1000000  # 取当前时间戳模一个数，避免太大\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'max_depth': '12',\n",
    "    'num_leaves': 63,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 10,\n",
    "    'verbose': -1,\n",
    "    'n_jobs':8,\n",
    "    'seed': seed  # 使用动态生成的 seed\n",
    "}\n",
    "\n",
    "# 五折交叉验证，使用五折构建特征时的切分规则，保证切分一致\n",
    "n_folds = 5\n",
    "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "test_preds = np.zeros(len(X_test))\n",
    "fold_thresholds = []\n",
    "fold_f1_scores = []\n",
    "models = []\n",
    "oof_preds = np.zeros(len(X_train))\n",
    "oof_probas = np.zeros(len(X_train))\n",
    "\n",
    "print(\"\\n开始模型训练...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "    print(f\"\\n======= Fold {fold+1}/{n_folds} =======\")\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # 创建数据集（指定类别特征）\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_set = lgb.Dataset(X_val, label=y_val)\n",
    "    \n",
    "    # 模型训练\n",
    "    model = lgb.train(\n",
    "        params,train_set,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_set, val_set],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    models.append(model)\n",
    "    \n",
    "    # 验证集预测\n",
    "    val_pred_proba = model.predict(X_val)\n",
    "    oof_probas[val_idx] = val_pred_proba\n",
    "    \n",
    "    # 阈值优化\n",
    "    best_threshold, best_f1 = find_optimal_threshold(y_val, val_pred_proba)\n",
    "    fold_thresholds.append(best_threshold)\n",
    "    \n",
    "    # 使用优化阈值计算F1\n",
    "    val_pred_labels = (val_pred_proba >= best_threshold).astype(int)\n",
    "    fold_f1 = f1_score(y_val, val_pred_labels)\n",
    "    fold_f1_scores.append(fold_f1)\n",
    "    oof_preds[val_idx] = val_pred_labels\n",
    "    \n",
    "    print(f\"Fold {fold+1} Optimal Threshold: {best_threshold:.4f}\")\n",
    "    print(f\"Fold {fold+1} F1 Score: {fold_f1:.5f}\")\n",
    "    \n",
    "    # 测试集预测\n",
    "    test_preds += model.predict(X_test) / n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec864a-519c-477b-93a6-0c5649d9d655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-03T15:43:34.208190Z",
     "iopub.status.busy": "2025-07-03T15:43:34.207954Z",
     "iopub.status.idle": "2025-07-03T15:43:35.528306Z",
     "shell.execute_reply": "2025-07-03T15:43:35.527844Z",
     "shell.execute_reply.started": "2025-07-03T15:43:34.208173Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Final Results =====\n",
      "Average Optimal Threshold: 0.4000\n",
      "Fold F1 Scores: ['0.85934', '0.85533', '0.85882', '0.85543', '0.85836']\n",
      "Average Fold F1: 0.85746\n",
      "OOF F1 Score: 0.85746\n",
      "\n",
      "Submission file saved: submit.csv\n",
      "Predicted new user ratio: 0.1722\n",
      "Test set size: 1143309\n",
      "\n",
      "Top 10 Features:\n",
      "                    Feature    Importance\n",
      "21  action_timespan_seconds  2.105181e+06\n",
      "19          first_action_ts  1.698942e+06\n",
      "15                common_ts  1.026575e+06\n",
      "14                      day  4.083276e+05\n",
      "20           last_action_ts  3.491723e+05\n",
      "8                    appver  3.410551e+05\n",
      "17                frequency  3.158639e+05\n",
      "18              mid_nunique  2.499437e+05\n",
      "0                       mid  2.478179e+05\n",
      "9                   channel  2.079482e+05\n",
      "7               common_city  1.378716e+05\n",
      "12                     hour  1.189031e+05\n",
      "10                  os_type  1.062723e+05\n",
      "6           common_province  9.473420e+04\n",
      "2              device_brand  8.416809e+04\n",
      "16                  recency  7.314627e+04\n",
      "13                dayofweek  6.424208e+04\n",
      "1                       eid  5.449029e+04\n",
      "4                  operator  4.552801e+04\n",
      "3                       ntt  2.381148e+04\n",
      "11                    udmap  6.894202e+03\n",
      "5            common_country  2.096370e+03\n"
     ]
    }
   ],
   "source": [
    "# 8. 整体结果评估\n",
    "# 使用交叉验证平均阈值\n",
    "avg_threshold = np.mean(fold_thresholds)\n",
    "final_oof_preds = (oof_probas >= avg_threshold).astype(int)\n",
    "final_f1 = f1_score(y_train, final_oof_preds)\n",
    "\n",
    "print(\"\\n===== Final Results =====\")\n",
    "print(f\"Average Optimal Threshold: {avg_threshold:.4f}\")\n",
    "print(f\"Fold F1 Scores: {[f'{s:.5f}' for s in fold_f1_scores]}\")\n",
    "print(f\"Average Fold F1: {np.mean(fold_f1_scores):.5f}\")\n",
    "print(f\"OOF F1 Score: {final_f1:.5f}\")\n",
    "\n",
    "# 9. 测试集预测与提交文件生成\n",
    "# 使用平均阈值进行预测\n",
    "test_pred_labels = (test_preds >= avg_threshold).astype(int)\n",
    "submit['is_new_did'] = test_pred_labels\n",
    "\n",
    "# 保存提交文件\n",
    "submit[['is_new_did']].to_csv('submit.csv', index=False)\n",
    "print(\"\\nSubmission file saved: submit.csv\")\n",
    "print(f\"Predicted new user ratio: {test_pred_labels.mean():.4f}\")\n",
    "print(f\"Test set size: {len(test_pred_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b805313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Features:\n",
      "                    Feature    Importance\n",
      "21  action_timespan_seconds  2.105181e+06\n",
      "19          first_action_ts  1.698942e+06\n",
      "15                common_ts  1.026575e+06\n",
      "14                      day  4.083276e+05\n",
      "20           last_action_ts  3.491723e+05\n",
      "8                    appver  3.410551e+05\n",
      "17                frequency  3.158639e+05\n",
      "18              mid_nunique  2.499437e+05\n",
      "0                       mid  2.478179e+05\n",
      "9                   channel  2.079482e+05\n",
      "7               common_city  1.378716e+05\n",
      "12                     hour  1.189031e+05\n",
      "10                  os_type  1.062723e+05\n",
      "6           common_province  9.473420e+04\n",
      "2              device_brand  8.416809e+04\n",
      "16                  recency  7.314627e+04\n",
      "13                dayofweek  6.424208e+04\n",
      "1                       eid  5.449029e+04\n",
      "4                  operator  4.552801e+04\n",
      "3                       ntt  2.381148e+04\n",
      "11                    udmap  6.894202e+03\n",
      "5            common_country  2.096370e+03\n"
     ]
    }
   ],
   "source": [
    "# 10. 特征重要性分析\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': models[0].feature_importance(importance_type='gain')\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "print(feature_importance.head(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
