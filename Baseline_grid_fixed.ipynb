{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "训练集大小: (3429925, 15)\n",
            "测试集大小: (1143309, 14)\n",
            "训练集和测试集中重叠的did数量: 192393\n",
            "重叠比例: 0.9324\n",
            "CPU times: total: 7.75 s\n",
            "Wall time: 7.75 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 1. 数据加载\n",
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./testA_data.csv')\n",
        "submit = test_df[['did']]\n",
        "\n",
        "print(f\"训练集大小: {train_df.shape}\")\n",
        "print(f\"测试集大小: {test_df.shape}\")\n",
        "\n",
        "# 检查训练集和测试集中的did是否有重叠\n",
        "train_dids = set(train_df['did'])\n",
        "test_dids = set(test_df['did'])\n",
        "common_dids = train_dids & test_dids\n",
        "print(f\"训练集和测试集中重叠的did数量: {len(common_dids)}\")\n",
        "print(f\"重叠比例: {len(common_dids)/len(test_dids):.4f}\")\n",
        "\n",
        "# 2. 时间特征工程\n",
        "for df in [train_df, test_df]:\n",
        "    # 转换为时间戳\n",
        "    df['ts'] = pd.to_datetime(df['common_ts'], unit='ms')\n",
        "    \n",
        "    # 提取时间特征\n",
        "    df['day'] = df['ts'].dt.day\n",
        "    df['dayofweek'] = df['ts'].dt.dayofweek\n",
        "    df['hour'] = df['ts'].dt.hour\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "修复数据泄露：仅使用训练集计算RFM特征...\n",
            "训练集中RFM特征缺失值情况:\n",
            "recency                    0\n",
            "frequency                  0\n",
            "mid_nunique                0\n",
            "first_action_ts            0\n",
            "last_action_ts             0\n",
            "action_timespan_seconds    0\n",
            "dtype: int64\n",
            "\n",
            "测试集中RFM特征缺失值情况:\n",
            "recency                    0\n",
            "frequency                  0\n",
            "mid_nunique                0\n",
            "first_action_ts            0\n",
            "last_action_ts             0\n",
            "action_timespan_seconds    0\n",
            "dtype: int64\n",
            "CPU times: total: 15.2 s\n",
            "Wall time: 15.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 3. RFM 特征工程 - 修复数据泄露问题\n",
        "print(\"修复数据泄露：仅使用训练集计算RFM特征...\")\n",
        "\n",
        "# 3.1 仅在训练集上计算RFM特征\n",
        "max_ts_train = train_df['ts'].max()\n",
        "rfm_agg_train = train_df.groupby('did').agg({\n",
        "    'ts': lambda x: (max_ts_train - x.max()).days,  # Recency\n",
        "    'eid': 'count',  # Frequency\n",
        "    'mid': 'nunique',  # 行为深度\n",
        "    'common_ts': ['min', 'max']  # 首次和末次行为时间\n",
        "})\n",
        "\n",
        "# 3.2 列名扁平化处理\n",
        "rfm_agg_train.columns = ['_'.join(col).strip() for col in rfm_agg_train.columns.values]\n",
        "rfm_agg_train = rfm_agg_train.reset_index()\n",
        "\n",
        "# 3.3 特征重命名\n",
        "rfm_agg_train.rename(columns={\n",
        "    'ts_<lambda>': 'recency',\n",
        "    'eid_count': 'frequency',\n",
        "    'mid_nunique': 'mid_nunique',\n",
        "    'common_ts_min': 'first_action_ts',\n",
        "    'common_ts_max': 'last_action_ts'\n",
        "}, inplace=True)\n",
        "\n",
        "# 3.4 派生新特征\n",
        "rfm_agg_train['action_timespan_seconds'] = (rfm_agg_train['last_action_ts'] - rfm_agg_train['first_action_ts']) / 1000\n",
        "\n",
        "# 3.5 合并RFM特征\n",
        "# 训练集：直接合并\n",
        "train_df = pd.merge(train_df, rfm_agg_train, on='did', how='left')\n",
        "\n",
        "# 测试集：只对在训练集中出现过的did合并特征，新did的RFM特征设为默认值\n",
        "test_df = pd.merge(test_df, rfm_agg_train, on='did', how='left')\n",
        "\n",
        "# 对于测试集中新的did，填充RFM特征的默认值\n",
        "rfm_features = ['recency', 'frequency', 'mid_nunique', 'first_action_ts', 'last_action_ts', 'action_timespan_seconds']\n",
        "for feature in rfm_features:\n",
        "    if feature in ['recency']:\n",
        "        test_df[feature].fillna(train_df[feature].median(), inplace=True)\n",
        "    elif feature in ['frequency', 'mid_nunique']:\n",
        "        test_df[feature].fillna(train_df[feature].median(), inplace=True)\n",
        "    else:\n",
        "        test_df[feature].fillna(train_df[feature].median(), inplace=True)\n",
        "\n",
        "print(f\"训练集中RFM特征缺失值情况:\")\n",
        "print(train_df[rfm_features].isnull().sum())\n",
        "print(f\"\\n测试集中RFM特征缺失值情况:\")\n",
        "print(test_df[rfm_features].isnull().sum())\n",
        "\n",
        "# 3.6 清理不再需要的 ts 列\n",
        "for df in [train_df, test_df]:\n",
        "    df.drop(['ts'], axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "每一列的数据类型:\n",
            "mid                          int64\n",
            "eid                          int64\n",
            "did                         object\n",
            "device_brand                 int32\n",
            "ntt                          int32\n",
            "operator                     int32\n",
            "common_country               int32\n",
            "common_province              int32\n",
            "common_city                  int32\n",
            "appver                       int32\n",
            "channel                      int32\n",
            "common_ts                    int64\n",
            "os_type                      int32\n",
            "udmap                        int32\n",
            "is_new_did                   int64\n",
            "day                          int32\n",
            "dayofweek                    int32\n",
            "hour                         int32\n",
            "recency                      int64\n",
            "frequency                    int64\n",
            "mid_nunique                  int64\n",
            "first_action_ts              int64\n",
            "last_action_ts               int64\n",
            "action_timespan_seconds    float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(\"每一列的数据类型:\")\n",
        "print(train_df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 17.6 s\n",
            "Wall time: 17.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 4. 类别特征编码\n",
        "cat_features = [\n",
        "    'device_brand', 'ntt', 'operator', 'common_country',\n",
        "    'common_province', 'common_city', 'appver', 'channel',\n",
        "    'os_type', 'udmap'\n",
        "]\n",
        "\n",
        "label_encoders = {}\n",
        "\n",
        "for feature in cat_features:\n",
        "    le = LabelEncoder()\n",
        "    \n",
        "    # 合并训练集和测试集的所有类别\n",
        "    all_values = pd.concat([train_df[feature], test_df[feature]]).astype(str)\n",
        "    \n",
        "    le.fit(all_values)\n",
        "    label_encoders[feature] = le\n",
        "    \n",
        "    # 应用编码\n",
        "    train_df[feature] = le.transform(train_df[feature].astype(str))\n",
        "    test_df[feature] = le.transform(test_df[feature].astype(str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用的特征数量: 22\n",
            "特征列表: ['mid', 'eid', 'device_brand', 'ntt', 'operator', 'common_country', 'common_province', 'common_city', 'appver', 'channel', 'os_type', 'udmap', 'hour', 'dayofweek', 'day', 'common_ts', 'recency', 'frequency', 'mid_nunique', 'first_action_ts', 'last_action_ts', 'action_timespan_seconds']\n",
            "\n",
            "重要提示: did未被用作特征，RFM特征仅基于训练集计算\n",
            "CPU times: total: 188 ms\n",
            "Wall time: 178 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 5. 特征准备 - 注意：did不包含在特征中！\n",
        "features = [\n",
        "    # 原始特征\n",
        "    'mid', 'eid', 'device_brand', 'ntt', 'operator', \n",
        "    'common_country', 'common_province', 'common_city',\n",
        "    'appver', 'channel', 'os_type', 'udmap',\n",
        "    # 时间特征\n",
        "    'hour', 'dayofweek', 'day', 'common_ts',\n",
        "    # RFM特征（基于训练集计算，避免数据泄露）\n",
        "    'recency', 'frequency', 'mid_nunique', 'first_action_ts', 'last_action_ts', 'action_timespan_seconds'\n",
        "]\n",
        "\n",
        "print(f\"使用的特征数量: {len(features)}\")\n",
        "print(f\"特征列表: {features}\")\n",
        "print(f\"\\n重要提示: did未被用作特征，RFM特征仅基于训练集计算\")\n",
        "\n",
        "# 准备训练和测试数据\n",
        "X_train = train_df[features]\n",
        "y_train = train_df['is_new_did']\n",
        "X_test = test_df[features]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# F1阈值优化函数\n",
        "def find_optimal_threshold(y_true, y_pred_proba):\n",
        "    \"\"\"寻找最大化F1分数的阈值\"\"\"\n",
        "    best_threshold = 0.5\n",
        "    best_f1 = 0\n",
        "    \n",
        "    for threshold in [0.1,0.15,0.2,0.25,0.3,0.35,0.4]:\n",
        "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_threshold = threshold\n",
        "    \n",
        "    return best_threshold, best_f1\n",
        "\n",
        "# 模型参数\n",
        "optimal_params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'verbose': -1,\n",
        "    'n_jobs': 8,\n",
        "    'seed': 42,\n",
        "    'max_depth': 15,\n",
        "    'num_leaves': 255,\n",
        "    'learning_rate': 0.1,\n",
        "    'min_child_samples': 10,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 3\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始训练修复数据泄露后的模型...\n",
            "\n",
            "======= Fold 1/5 =======\n",
            "[100]\ttraining's binary_logloss: 0.133373\tvalid_1's binary_logloss: 0.13612\n",
            "[200]\ttraining's binary_logloss: 0.111356\tvalid_1's binary_logloss: 0.115919\n",
            "[300]\ttraining's binary_logloss: 0.0978509\tvalid_1's binary_logloss: 0.103994\n",
            "[400]\ttraining's binary_logloss: 0.0870139\tvalid_1's binary_logloss: 0.0944971\n",
            "[500]\ttraining's binary_logloss: 0.0777062\tvalid_1's binary_logloss: 0.086248\n",
            "[600]\ttraining's binary_logloss: 0.0706348\tvalid_1's binary_logloss: 0.0802052\n",
            "[700]\ttraining's binary_logloss: 0.0641976\tvalid_1's binary_logloss: 0.0746866\n",
            "[800]\ttraining's binary_logloss: 0.0585294\tvalid_1's binary_logloss: 0.0699452\n",
            "[900]\ttraining's binary_logloss: 0.0536773\tvalid_1's binary_logloss: 0.0659131\n",
            "[1000]\ttraining's binary_logloss: 0.0488009\tvalid_1's binary_logloss: 0.0616269\n",
            "Fold 1 Optimal Threshold: 0.4000\n",
            "Fold 1 F1 Score: 0.93618\n",
            "\n",
            "======= Fold 2/5 =======\n",
            "[100]\ttraining's binary_logloss: 0.132954\tvalid_1's binary_logloss: 0.135626\n",
            "[200]\ttraining's binary_logloss: 0.111458\tvalid_1's binary_logloss: 0.116111\n",
            "[300]\ttraining's binary_logloss: 0.0970943\tvalid_1's binary_logloss: 0.103402\n",
            "[400]\ttraining's binary_logloss: 0.0864577\tvalid_1's binary_logloss: 0.0940551\n",
            "[500]\ttraining's binary_logloss: 0.0776959\tvalid_1's binary_logloss: 0.0865763\n",
            "[600]\ttraining's binary_logloss: 0.0699217\tvalid_1's binary_logloss: 0.0797914\n",
            "[700]\ttraining's binary_logloss: 0.0639236\tvalid_1's binary_logloss: 0.0747674\n",
            "[800]\ttraining's binary_logloss: 0.0579034\tvalid_1's binary_logloss: 0.0695322\n",
            "[900]\ttraining's binary_logloss: 0.0530652\tvalid_1's binary_logloss: 0.0655787\n",
            "[1000]\ttraining's binary_logloss: 0.0487161\tvalid_1's binary_logloss: 0.0619425\n",
            "Fold 2 Optimal Threshold: 0.4000\n",
            "Fold 2 F1 Score: 0.93472\n",
            "\n",
            "======= Fold 3/5 =======\n",
            "[100]\ttraining's binary_logloss: 0.133735\tvalid_1's binary_logloss: 0.135645\n",
            "[200]\ttraining's binary_logloss: 0.112229\tvalid_1's binary_logloss: 0.116197\n",
            "[300]\ttraining's binary_logloss: 0.0975374\tvalid_1's binary_logloss: 0.103231\n",
            "[400]\ttraining's binary_logloss: 0.0861443\tvalid_1's binary_logloss: 0.0931824\n",
            "[500]\ttraining's binary_logloss: 0.0769602\tvalid_1's binary_logloss: 0.0852648\n",
            "[600]\ttraining's binary_logloss: 0.0694062\tvalid_1's binary_logloss: 0.0788846\n",
            "[700]\ttraining's binary_logloss: 0.0632199\tvalid_1's binary_logloss: 0.0736464\n",
            "[800]\ttraining's binary_logloss: 0.057693\tvalid_1's binary_logloss: 0.0689748\n",
            "[900]\ttraining's binary_logloss: 0.0529006\tvalid_1's binary_logloss: 0.0650185\n",
            "[1000]\ttraining's binary_logloss: 0.0485428\tvalid_1's binary_logloss: 0.061394\n",
            "Fold 3 Optimal Threshold: 0.4000\n",
            "Fold 3 F1 Score: 0.93644\n",
            "\n",
            "======= Fold 4/5 =======\n",
            "[100]\ttraining's binary_logloss: 0.13322\tvalid_1's binary_logloss: 0.136151\n",
            "[200]\ttraining's binary_logloss: 0.111586\tvalid_1's binary_logloss: 0.116309\n",
            "[300]\ttraining's binary_logloss: 0.0972015\tvalid_1's binary_logloss: 0.103417\n",
            "[400]\ttraining's binary_logloss: 0.0861731\tvalid_1's binary_logloss: 0.0937664\n",
            "[500]\ttraining's binary_logloss: 0.0777297\tvalid_1's binary_logloss: 0.0865631\n",
            "[600]\ttraining's binary_logloss: 0.0701723\tvalid_1's binary_logloss: 0.0800637\n",
            "[700]\ttraining's binary_logloss: 0.0636652\tvalid_1's binary_logloss: 0.0745191\n",
            "[800]\ttraining's binary_logloss: 0.0582722\tvalid_1's binary_logloss: 0.069998\n",
            "[900]\ttraining's binary_logloss: 0.0533129\tvalid_1's binary_logloss: 0.0658509\n",
            "[1000]\ttraining's binary_logloss: 0.0488891\tvalid_1's binary_logloss: 0.0620557\n",
            "Fold 4 Optimal Threshold: 0.4000\n",
            "Fold 4 F1 Score: 0.93466\n",
            "\n",
            "======= Fold 5/5 =======\n",
            "[100]\ttraining's binary_logloss: 0.133157\tvalid_1's binary_logloss: 0.135318\n",
            "[200]\ttraining's binary_logloss: 0.111963\tvalid_1's binary_logloss: 0.116076\n",
            "[300]\ttraining's binary_logloss: 0.0978278\tvalid_1's binary_logloss: 0.103743\n",
            "[400]\ttraining's binary_logloss: 0.0868618\tvalid_1's binary_logloss: 0.0942263\n",
            "[500]\ttraining's binary_logloss: 0.0782675\tvalid_1's binary_logloss: 0.0869003\n",
            "[600]\ttraining's binary_logloss: 0.0706818\tvalid_1's binary_logloss: 0.0802849\n",
            "[700]\ttraining's binary_logloss: 0.0640457\tvalid_1's binary_logloss: 0.0744815\n",
            "[800]\ttraining's binary_logloss: 0.0584273\tvalid_1's binary_logloss: 0.0696569\n",
            "[900]\ttraining's binary_logloss: 0.0534881\tvalid_1's binary_logloss: 0.0654983\n",
            "[1000]\ttraining's binary_logloss: 0.0490018\tvalid_1's binary_logloss: 0.0617277\n",
            "Fold 5 Optimal Threshold: 0.4000\n",
            "Fold 5 F1 Score: 0.93542\n",
            "CPU times: total: 1h 11min 23s\n",
            "Wall time: 8min 24s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# 模型训练（修复数据泄露后）\n",
        "print(\"开始训练修复数据泄露后的模型...\")\n",
        "\n",
        "n_folds = 5\n",
        "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "test_preds = np.zeros(len(X_test))\n",
        "fold_thresholds = []\n",
        "fold_f1_scores = []\n",
        "models = []\n",
        "oof_preds = np.zeros(len(X_train))\n",
        "oof_probas = np.zeros(len(X_train))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n",
        "    print(f\"\\n======= Fold {fold+1}/{n_folds} =======\")\n",
        "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "    \n",
        "    # 创建数据集\n",
        "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
        "    val_set = lgb.Dataset(X_val, label=y_val)\n",
        "    \n",
        "    # 模型训练\n",
        "    model = lgb.train(\n",
        "        optimal_params, train_set,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[train_set, val_set],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
        "            lgb.log_evaluation(period=100)\n",
        "        ]\n",
        "    )\n",
        "    models.append(model)\n",
        "    \n",
        "    # 验证集预测\n",
        "    val_pred_proba = model.predict(X_val)\n",
        "    oof_probas[val_idx] = val_pred_proba\n",
        "    \n",
        "    # 阈值优化\n",
        "    best_threshold, best_f1 = find_optimal_threshold(y_val, val_pred_proba)\n",
        "    fold_thresholds.append(best_threshold)\n",
        "    \n",
        "    # 使用优化阈值计算F1\n",
        "    val_pred_labels = (val_pred_proba >= best_threshold).astype(int)\n",
        "    fold_f1 = f1_score(y_val, val_pred_labels)\n",
        "    fold_f1_scores.append(fold_f1)\n",
        "    oof_preds[val_idx] = val_pred_labels\n",
        "    \n",
        "    print(f\"Fold {fold+1} Optimal Threshold: {best_threshold:.4f}\")\n",
        "    print(f\"Fold {fold+1} F1 Score: {fold_f1:.5f}\")\n",
        "    \n",
        "    # 测试集预测\n",
        "    test_preds += model.predict(X_test) / n_folds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== 修复数据泄露后的最终结果 =====\n",
            "Average Optimal Threshold: 0.4000\n",
            "Fold F1 Scores: ['0.93618', '0.93472', '0.93644', '0.93466', '0.93542']\n",
            "Average Fold F1: 0.93549\n",
            "OOF F1 Score: 0.93549\n",
            "\n",
            "修复数据泄露后的提交文件已保存: submit_fixed.csv\n",
            "预测新用户比例: 0.1565\n",
            "测试集大小: 1143309\n",
            "\n",
            "===== 数据泄露修复总结 =====\n",
            "1. ✅ did未被用作特征\n",
            "2. ✅ RFM特征仅基于训练集计算\n",
            "3. ✅ 测试集中新did的RFM特征用训练集统计值填充\n",
            "4. ✅ 避免了future information leakage\n"
          ]
        }
      ],
      "source": [
        "# 整体结果评估\n",
        "avg_threshold = np.mean(fold_thresholds)\n",
        "final_oof_preds = (oof_probas >= avg_threshold).astype(int)\n",
        "final_f1 = f1_score(y_train, final_oof_preds)\n",
        "\n",
        "print(\"\\n===== 修复数据泄露后的最终结果 =====\")\n",
        "print(f\"Average Optimal Threshold: {avg_threshold:.4f}\")\n",
        "print(f\"Fold F1 Scores: {[f'{s:.5f}' for s in fold_f1_scores]}\")\n",
        "print(f\"Average Fold F1: {np.mean(fold_f1_scores):.5f}\")\n",
        "print(f\"OOF F1 Score: {final_f1:.5f}\")\n",
        "\n",
        "# 测试集预测与提交文件生成\n",
        "test_pred_labels = (test_preds >= avg_threshold).astype(int)\n",
        "submit['is_new_did'] = test_pred_labels\n",
        "\n",
        "# 保存提交文件\n",
        "submit[['is_new_did']].to_csv('submit_fixed.csv', index=False)\n",
        "print(\"\\n修复数据泄露后的提交文件已保存: submit_fixed.csv\")\n",
        "print(f\"预测新用户比例: {test_pred_labels.mean():.4f}\")\n",
        "print(f\"测试集大小: {len(test_pred_labels)}\")\n",
        "\n",
        "print(\"\\n===== 数据泄露修复总结 =====\")\n",
        "print(\"1. ✅ did未被用作特征\")\n",
        "print(\"2. ✅ RFM特征仅基于训练集计算\")\n",
        "print(\"3. ✅ 测试集中新did的RFM特征用训练集统计值填充\")\n",
        "print(\"4. ✅ 避免了future information leakage\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
